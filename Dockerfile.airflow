# Use the Apache Airflow base image
FROM apache/airflow:2.7.1-python3.11

USER root

# Install system dependencies
RUN apt-get update && \
    apt-get install -y procps && \
    apt-get install -y gcc python3-dev openjdk-11-jdk && \
    apt-get clean
# Download and install Spark
ENV SPARK_VERSION=3.2.1
ENV HADOOP_VERSION=3.2
RUN curl -O https://archive.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz && \
    tar xf spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz -C /usr/local && \
    rm spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz && \
    ln -s /usr/local/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION /usr/local/spark


# Set environment variables for Spark
ENV SPARK_HOME=/usr/local/spark
ENV PATH=$PATH:$SPARK_HOME/bin

USER airflow
# Install Apache Airflow and other Python dependencies
#RUN pip install apache-airflow apache-airflow-providers-apache-spark
COPY requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt

# Expose necessary Airflow ports
EXPOSE 8080 5555 8793

WORKDIR /opt/airflow

CMD ["bash"]
